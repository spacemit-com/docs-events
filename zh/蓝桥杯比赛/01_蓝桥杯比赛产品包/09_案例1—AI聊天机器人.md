# 环境搭建

## 下载代码

下载源码压缩包：[asr-llm-tts.zip](code/ai_demo/asr-llm-tts.zip)

解压命令：

```bash
unzip asr-llm-tts.zip -d ~/
```

## 安装系统依赖

```
sudo apt update
sudo apt install -y python3-spacemit-ort spacemit-ollama-toolkit virtualenv wget
```

## 安装Python依赖

（1）创建Python虚拟环境

```
virtualenv ~/demo-env
```

（2）配置 pip 源为进迭时空镜像源

```
pip config set global.extra-index-url https://git.spacemit.com/api/v4/projects/33/packages/pypi/simple
```

（3）安装项目依赖

```
cd ~/asr-llm-tts
source ~/demo-env/bin/activate
pip install opencv-python==4.6.8.1
pip install pillow==11.2.1
```

## 将LLM模型部署到Ollama

**（1）确保Ollama服务在后台运行**

执行以下命令检查服务状态：

```
systemctl status ollama
```

输出应该如下所示：

![image-20250422140656034](images/ai-chat-ollama-status.png)

若状态为 `inactive`，则使用以下命令启动服务：

```
systemctl start ollama
```

**（2）下载Deepseek模型及配置文件**

```
mkdir -p ~/my-ollama
cd ~/my-ollama
wget https://archive.spacemit.com/spacemit-ai/openwebui/deepseek-r1-1.5b.modelfile
wget https://www.modelscope.cn/models/ggml-org/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0-GGUF/resolve/master/deepseek-r1-distill-qwen-1.5b-q4_0.gguf
```

**（3）将模型加载至Ollama**

```
ollama create deepseek-r1-1.5b -f deepseek-r1-1.5b.modelfile
```

**（4）复制NLTK数据**

```
cp -r ~/asr-llm-tts/nltk_data ~/
```

# 程序启动

- 进入项目目录并激活虚拟环境

```
cd ~/asr-llm-tts/src
source ~/demo-env/bin/activate
```

- 启动语音输出版本

```
python main_tts.py
```

- 启动文本输出版本

```
python main.py
```

